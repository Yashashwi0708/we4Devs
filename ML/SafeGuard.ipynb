{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGu8dkLV04e8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset with 'latin1' encoding\n",
        "df = pd.read_csv('spam_dataset.csv', encoding='latin1')\n",
        "\n",
        "# Split the dataset into features and labels\n",
        "X = df['sms']\n",
        "y = df['label']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Convert text data into numerical data using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(X).toarray()  # Convert sparse matrix to dense array\n"
      ],
      "metadata": {
        "id": "pgSJQgmu2ewo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use SMOTE to handle imbalanced dataset\n",
        "# from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "qmoRt66f3zwT",
        "outputId": "028104f8-46dd-48c3-ef74-7475a6d8f895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'SMOTE' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-989c9fcdaf82>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use SMOTE to handle imbalanced dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# from imblearn.over_sampling import SMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msmote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SMOTE' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, optim, tensor\n",
        "from torch.nn import functional as F\n",
        "import time\n",
        "\n",
        "import pandas\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from torch import nn, optim, tensor\n",
        "from torch.nn import functional as F\n"
      ],
      "metadata": {
        "id": "Sxu75h8d36vs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_labels, dropout):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_labels = num_labels\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.dp = nn.Dropout(self.dropout)\n",
        "        self.ff = nn.Linear(self.embedding_dim, self.num_labels)\n",
        "\n",
        "    def forward(self, input_embeddings):\n",
        "        tensor = self.dp(input_embeddings)\n",
        "        tensor = self.ff(tensor)\n",
        "        return tensor, F.softmax(tensor, dim=-1)\n",
        "\n",
        "\n",
        "class Batcher(object):\n",
        "    def __init__(self, data_x, data_y, batch_size):\n",
        "        self.data_x = data_x\n",
        "        self.data_y = data_y\n",
        "        self.batch_size = batch_size\n",
        "        self.n_samples = data_x.shape[0]\n",
        "        self.indices = torch.randperm(self.n_samples)\n",
        "        self.ptr = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.ptr > self.n_samples:\n",
        "            self.ptr = 0\n",
        "            self.indices = torch.randperm(self.n_samples)\n",
        "            raise StopIteration\n",
        "        else:\n",
        "            batch_indices = self.indices[self.ptr:self.ptr+self.batch_size]\n",
        "            self.ptr += self.batch_size\n",
        "            return self.data_x[batch_indices], self.data_y[batch_indices]\n",
        "\n",
        "\n",
        "data_df = pandas.read_csv(\"/content/data-en-hi-de-fr.csv\")\n",
        "\n",
        "data_df.dropna(inplace=True)\n",
        "data_df.drop_duplicates(inplace=True)\n",
        "data_df.rename(columns={\n",
        "    \"Category\": \"labels\",\n",
        "    \"Message\": \"text\"\n",
        "}, inplace=True)\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(data_df.labels)\n",
        "data_df[\"labels\"] = le.transform(data_df.labels)\n",
        "\n",
        "train_x, test_x, train_y, test_y = \\\n",
        "    train_test_split(data_df.text, data_df.labels, stratify=data_df.labels, test_size=0.15,\n",
        "                     random_state=123)\n",
        "\n",
        "train_x_de, test_x_de, train_y_de, test_y_de = \\\n",
        "    train_test_split(data_df.text_hi, data_df.labels, stratify=data_df.labels, test_size=0.15,\n",
        "                     random_state=123)\n",
        "\n",
        "sentences = train_x.tolist()\n",
        "test_sentences = test_x_de.tolist()\n",
        "\n",
        "labels = torch.tensor(train_y.tolist())\n",
        "test_labels = torch.tensor(test_y_de.tolist())\n",
        "\n",
        "# encoder = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "encoder = SentenceTransformer('quora-distilbert-multilingual')\n",
        "print('Encoding segments...')\n",
        "start = time.time()\n",
        "embedding = encoder.encode(sentences, convert_to_tensor=True)\n",
        "test_sentences_embedding = encoder.encode(test_sentences, convert_to_tensor=True)\n",
        "print(f\"Encoding completed in {time.time() - start} seconds.\")\n",
        "\n",
        "train_batcher = Batcher(embedding, labels, batch_size=16)\n",
        "\n",
        "num_samples, embeddings_dim = embedding.size()\n",
        "n_labels = labels.unique().shape[0]\n",
        "\n",
        "classifier = Classifier(embeddings_dim, n_labels, dropout=0.01)\n",
        "\n",
        "optimizer = optim.Adam(classifier.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(10):\n",
        "    total_loss = 0\n",
        "    for batch in train_batcher:\n",
        "        x, y = batch\n",
        "        optimizer.zero_grad()\n",
        "        model_output, prob = classifier(x)\n",
        "        loss = loss_fn(model_output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f'epoch:{e}, total_loss:{total_loss}')\n",
        "\n",
        "with torch.no_grad():\n",
        "    model_output, prob = classifier(test_sentences_embedding)\n",
        "    predictions = torch.argmax(prob, dim=-1)\n",
        "    results = classification_report(predictions, test_labels)\n",
        "    print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBROhbSoTsBt",
        "outputId": "399b84a6-e705-4da9-ccbb-96219d08ccb1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding segments...\n",
            "Encoding completed in 353.12576961517334 seconds.\n",
            "epoch:0, total_loss:60.209235444664955\n",
            "epoch:1, total_loss:34.170039696618915\n",
            "epoch:2, total_loss:26.798776485025883\n",
            "epoch:3, total_loss:23.24302877113223\n",
            "epoch:4, total_loss:20.70875272154808\n",
            "epoch:5, total_loss:19.180852215271443\n",
            "epoch:6, total_loss:17.432323425775394\n",
            "epoch:7, total_loss:16.62215864774771\n",
            "epoch:8, total_loss:15.740114335436374\n",
            "epoch:9, total_loss:14.825828957837075\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.96       727\n",
            "           1       0.46      0.94      0.62        47\n",
            "\n",
            "    accuracy                           0.93       774\n",
            "   macro avg       0.73      0.93      0.79       774\n",
            "weighted avg       0.96      0.93      0.94       774\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "import time\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_labels, dropout):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_labels = num_labels\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.dp = nn.Dropout(self.dropout)\n",
        "        self.ff = nn.Linear(self.embedding_dim, self.num_labels)\n",
        "\n",
        "    def forward(self, input_embeddings):\n",
        "        tensor = self.dp(input_embeddings)\n",
        "        tensor = self.ff(tensor)\n",
        "        return tensor, F.softmax(tensor, dim=-1)\n",
        "\n",
        "class Batcher(object):\n",
        "    def __init__(self, data_x, data_y, batch_size):\n",
        "        self.data_x = data_x\n",
        "        self.data_y = data_y\n",
        "        self.batch_size = batch_size\n",
        "        self.n_samples = data_x.shape[0]\n",
        "        self.indices = torch.randperm(self.n_samples)\n",
        "        self.ptr = 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.ptr >= self.n_samples:\n",
        "            self.ptr = 0\n",
        "            self.indices = torch.randperm(self.n_samples)\n",
        "            raise StopIteration\n",
        "        else:\n",
        "            batch_indices = self.indices[self.ptr:self.ptr+self.batch_size]\n",
        "            self.ptr += self.batch_size\n",
        "            return self.data_x[batch_indices], self.data_y[batch_indices]\n",
        "\n",
        "\n",
        "data_df = pd.read_csv(\"/content/data-en-hi-de-fr.csv\")\n",
        "\n",
        "data_df.dropna(inplace=True)\n",
        "data_df.drop_duplicates(inplace=True)\n",
        "data_df.rename(columns={\n",
        "    \"Category\": \"labels\",\n",
        "    \"Message\": \"text\"\n",
        "}, inplace=True)\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(data_df.labels)\n",
        "data_df[\"labels\"] = le.transform(data_df.labels)\n",
        "\n",
        "train_x, test_x, train_y, test_y = \\\n",
        "    train_test_split(data_df.text, data_df.labels, stratify=data_df.labels, test_size=0.15,\n",
        "                     random_state=123)\n",
        "\n",
        "train_x_de, test_x_de, train_y_de, test_y_de = \\\n",
        "    train_test_split(data_df.text, data_df.labels, stratify=data_df.labels, test_size=0.15,\n",
        "                     random_state=123)\n",
        "\n",
        "sentences = train_x.tolist()\n",
        "test_sentences = test_x_de.tolist()\n",
        "\n",
        "labels = train_y.to_numpy()\n",
        "test_labels = test_y_de.to_numpy()\n",
        "\n",
        "# encoder = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
        "encoder = SentenceTransformer('quora-distilbert-multilingual')\n",
        "print('Encoding segments...')\n",
        "start = time.time()\n",
        "embedding = encoder.encode(sentences, convert_to_tensor=True)\n",
        "\n",
        "# Applying SMOTE to the embeddings\n",
        "smote = SMOTE(random_state=42)\n",
        "oversampled_embeddings, oversampled_labels = smote.fit_resample(embedding, labels)\n",
        "\n",
        "test_sentences_embedding = encoder.encode(test_sentences, convert_to_tensor=True)\n",
        "print(f\"Encoding completed in {time.time() - start} seconds.\")\n",
        "\n",
        "train_batcher = Batcher(torch.tensor(oversampled_embeddings), torch.tensor(oversampled_labels), batch_size=16)\n",
        "\n",
        "num_samples, embeddings_dim = embedding.size()\n",
        "n_labels = torch.tensor(oversampled_labels).unique().shape[0]\n",
        "\n",
        "classifier = Classifier(embeddings_dim, n_labels, dropout=0.01)\n",
        "\n",
        "optimizer = optim.Adam(classifier.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for e in range(10):\n",
        "    total_loss = 0\n",
        "    for batch in train_batcher:\n",
        "        x, y = batch\n",
        "        optimizer.zero_grad()\n",
        "        model_output, prob = classifier(x)\n",
        "        loss = loss_fn(model_output, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f'epoch:{e}, total_loss:{total_loss}')\n",
        "\n",
        "with torch.no_grad():\n",
        "    model_output, prob = classifier(test_sentences_embedding)\n",
        "    predictions = torch.argmax(prob, dim=-1)\n",
        "    results = classification_report(predictions, test_labels)\n",
        "    print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v_1FBHRWEEz",
        "outputId": "6834db04-207b-4629-b23b-4cefe126670a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding segments...\n",
            "Encoding completed in 350.3312654495239 seconds.\n",
            "epoch:0, total_loss:117.23487184382975\n",
            "epoch:1, total_loss:55.81252033263445\n",
            "epoch:2, total_loss:42.591582499444485\n",
            "epoch:3, total_loss:35.13796570757404\n",
            "epoch:4, total_loss:30.69702981505543\n",
            "epoch:5, total_loss:27.116019108332694\n",
            "epoch:6, total_loss:24.74434843636118\n",
            "epoch:7, total_loss:23.39571693399921\n",
            "epoch:8, total_loss:21.975981851806864\n",
            "epoch:9, total_loss:20.27736062789336\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98       667\n",
            "           1       0.94      0.84      0.89       107\n",
            "\n",
            "    accuracy                           0.97       774\n",
            "   macro avg       0.96      0.92      0.93       774\n",
            "weighted avg       0.97      0.97      0.97       774\n",
            "\n"
          ]
        }
      ]
    }
  ]
}